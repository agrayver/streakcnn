{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training CNN for imaging fluid flow with streaks analysis\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepatory steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get some includes first and define convenience routines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import time\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchviz import make_dot\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.autograd import Variable\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from numpy import random\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fix random seed to make sure we get reproducible results. This can be removed at production stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_seed = 12345\n",
    "torch.manual_seed(my_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data and preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5f = h5py.File('data/train_val_sets_n=1M.h5','r')\n",
    "\n",
    "images = h5f['images'][()]\n",
    "Un = h5f['Un'][()]\n",
    "Phi = h5f['Phi'][()]\n",
    "windowSize = h5f['windowSize'][()]\n",
    "\n",
    "h5f.close()\n",
    "\n",
    "trainImages, validationImages, trainData, validationData = \\\n",
    "    train_test_split(images, np.column_stack((Un, Phi)), test_size=0.25, random_state=my_seed, shuffle = False)\n",
    "\n",
    "images = []\n",
    "\n",
    "indices = random.randint(0, len(trainImages), 16)\n",
    "\n",
    "fig = plt.figure()\n",
    "for i in range(indices.size):\n",
    "    ax = fig.add_subplot(4, 4, i + 1)\n",
    "    plt.gray()\n",
    "    plt.imshow(trainImages[indices[i],0,:,:])\n",
    "    ax.set_title(\"Un = %.1f, Phi = %.1f\" % (trainData[indices[i],0], trainData[indices[i],1]))\n",
    "    \n",
    "fig.set_size_inches(np.array(fig.get_size_inches()) * 3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "trainDataUnit = scaler.fit_transform(trainData)\n",
    "validationDataUnit = scaler.transform(validationData)\n",
    "\n",
    "print((trainDataUnit.shape, validationDataUnit.shape))\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1,2,1)\n",
    "plt.hist(trainDataUnit[:,0]);\n",
    "ax = fig.add_subplot(1,2,2)\n",
    "plt.hist(trainDataUnit[:,1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating and training of a CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some global settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load network architecture we will use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CNN_architectures import StreaksCNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert data into pytorch compatible format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'batch_size': 192,\n",
    "          'shuffle': True,\n",
    "          'num_workers': 0}\n",
    "\n",
    "xtrain = torch.from_numpy(trainImages / 255.).type(torch.FloatTensor)\n",
    "ytrain = torch.from_numpy(trainDataUnit).type(torch.FloatTensor)\n",
    "\n",
    "train_dataset = TensorDataset(xtrain, ytrain)\n",
    "train_loader = DataLoader(train_dataset, **params)\n",
    "\n",
    "#for batch_idx, (x, y) in enumerate(train_loader):\n",
    "#    print(x.shape, y.shape)\n",
    "\n",
    "xvalidation = torch.from_numpy(validationImages / 255.).type(torch.FloatTensor)\n",
    "yvalidation = torch.from_numpy(validationDataUnit).type(torch.FloatTensor)\n",
    "\n",
    "validation_dataset = TensorDataset(xvalidation, yvalidation)\n",
    "validation_loader = DataLoader(validation_dataset, **params)\n",
    "\n",
    "print(len(train_dataset), len(validation_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 3e-4\n",
    "\n",
    "net = StreaksCNN(winSz = windowSize, filtSz = 5, convPad = 2)\n",
    "net.to(device)\n",
    "loss = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n",
    "n_epochs = 40\n",
    "n_batches = len(train_loader)\n",
    "\n",
    "# Time for printing\n",
    "training_start_time = time.time()\n",
    "\n",
    "train_loss_log = np.zeros(n_epochs)\n",
    "validation_loss_log = np.zeros(n_epochs)\n",
    "\n",
    "# Loop for n_epochs\n",
    "for epoch in range(n_epochs):\n",
    "    print_every = n_batches // 5\n",
    "    start_time = time.time()\n",
    "    total_train_loss = 0\n",
    "        \n",
    "    net.train()\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        # Get inputs\n",
    "        inputs, labels = data\n",
    "            \n",
    "        # Wrap them in a Variable object\n",
    "        #inputs, labels = Variable(inputs), Variable(labels)\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "                \n",
    "        # Set the parameter gradients to zero\n",
    "        optimizer.zero_grad()\n",
    "            \n",
    "        # Forward pass, backward pass, optimize\n",
    "        outputs = net(inputs)\n",
    "        \n",
    "        loss_size = loss(outputs, labels)\n",
    "        loss_size.backward()\n",
    "        optimizer.step()\n",
    "            \n",
    "        total_train_loss += loss_size.item()\n",
    "\n",
    "        # Reset time\n",
    "        start_time = time.time()\n",
    "    \n",
    "    # At the end of the epoch, do a pass on the validation set\n",
    "    net.eval()\n",
    "    total_val_loss = 0\n",
    "    for inputs, labels in validation_loader:\n",
    "        # Wrap tensors in Variables\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        val_outputs = net(inputs)\n",
    "        val_loss_size = loss(val_outputs, labels)\n",
    "        total_val_loss += val_loss_size.item()\n",
    "\n",
    "    train_loss_log[epoch] = np.sqrt(total_train_loss / len(train_dataset))\n",
    "    validation_loss_log[epoch] = np.sqrt(total_val_loss / len(validation_dataset))\n",
    "    \n",
    "    print(\"Epoch %d. Train RMSE = {:.4f}, Validation RMSE = {:.4f}\".format(epoch, train_loss_log[epoch], validation_loss_log[epoch]))\n",
    "\n",
    "print(\"Training finished, took {:.4f}s\".format(time.time() - training_start_time))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the network is trained, let us display some learning statistics and model information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(train_loss_log, '')\n",
    "plt.plot(validation_loss_log)\n",
    "plt.legend(('Train', 'Validation'))\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('RMSE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply final network to the validation data set and check accuracy on Un and phi independently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'batch_size': 192,\n",
    "          'shuffle': False,\n",
    "          'num_workers': 0}\n",
    "\n",
    "test_loader = DataLoader(validation_dataset, **params)\n",
    "\n",
    "ypredicted = np.empty(shape=(0,2))\n",
    "\n",
    "net.eval()\n",
    "for inputs, outputs in test_loader:\n",
    "    # Wrap tensors in Variables\n",
    "    inputs, outputs = inputs.to(device), outputs.to(device)\n",
    "            \n",
    "    # Forward pass\n",
    "    val_outputs = net(inputs)\n",
    "    \n",
    "    if 'cuda' in device.type:\n",
    "        val_outputs = val_outputs.cpu()\n",
    "    \n",
    "    ypredicted = np.concatenate((ypredicted, val_outputs.data.numpy()))\n",
    "    \n",
    "ypredicted = scaler.inverse_transform(ypredicted)\n",
    "yresiduals = ypredicted - validationData\n",
    "yrmse = np.sqrt(np.mean(yresiduals**2, axis = 0))\n",
    "\n",
    "print('RMSE on Un = ', yrmse[0], ', RMSE on phi = ', yrmse[1])\n",
    "\n",
    "thrNorm = 2;\n",
    "thrPhi = 18; \n",
    "numCorrectNorm = np.sum(np.abs(yresiduals[:,0]) < thrNorm)\n",
    "numCorrectPhi = np.sum(np.abs(yresiduals[:,1]) < thrPhi)\n",
    "\n",
    "accuracy = np.array([numCorrectNorm, numCorrectPhi])/len(yresiduals)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save trained network and all additional training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_net = 'network_n=1M'\n",
    "\n",
    "torch.save(net.state_dict(), ('networks/%s.pytorch' % file_net))\n",
    "\n",
    "h5f = h5py.File(('networks/%s_data.h5' % file_net), 'w')\n",
    "h5f.create_dataset('RMSE', data=yrmse, compression=\"gzip\", compression_opts=6)\n",
    "#h5f.create_dataset('trainImages', data=trainImages, compression=\"gzip\", compression_opts=6)\n",
    "h5f.create_dataset('validationImages', data=validationImages, compression=\"gzip\", compression_opts=6)\n",
    "#h5f.create_dataset('trainData', data=trainData)\n",
    "h5f.create_dataset('validationData', data=validationData)\n",
    "h5f.create_dataset('scale', data=scaler.scale_)\n",
    "h5f.create_dataset('mean', data=scaler.mean_)\n",
    "h5f.create_dataset('rmse_training', data=train_loss_log)\n",
    "h5f.create_dataset('rmse_validation', data=validation_loss_log)\n",
    "h5f.create_dataset('windowSize', data=windowSize)\n",
    "\n",
    "h5f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
